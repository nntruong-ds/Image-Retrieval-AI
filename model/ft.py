# -*- coding: utf-8 -*-
"""FT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r127JZx-qHNe1ecrHtmvRuAebVEIKvi7
"""

import torch

if torch.cuda.is_available():
    print("‚úÖ GPU ƒëang b·∫≠t!")
    print("Lo·∫°i GPU:", torch.cuda.get_device_name(0))
else:
    print("‚ùå Kh√¥ng c√≥ GPU ‚Äî ƒëang ch·∫°y b·∫±ng CPU.")

from google.colab import drive
drive.mount('/content/drive')

import os
from PIL import Image
from tqdm import tqdm

def load_flickr8k_captions(caption_file):
    data = {}
    with open(caption_file, 'r') as f:
        for line in f:
            img, cap = line.strip().split(',', 1)
            if img not in data:
                data[img] = []
            data[img].append(cap)
    return data

captions_dict = load_flickr8k_captions("/content/drive/MyDrive/text_img/captions.txt")

import os
from torch.utils.data import Dataset, DataLoader
from transformers import CLIPProcessor
from PIL import Image
from PIL import UnidentifiedImageError
# Gi·∫£ ƒë·ªãnh: captions_dict ƒë√£ ƒë∆∞·ª£c t·∫£i/kh·ªüi t·∫°o tr∆∞·ªõc ƒë√≥
# V√≠ d·ª• v·ªÅ c·∫•u tr√∫c captions_dict:
# captions_dict = {
#     "image_file_1.jpg": ["caption 1a", "caption 1b", "caption 1c", "caption 1d", "caption 1e"],
#     "image_file_2.jpg": ["caption 2a", "caption 2b", "caption 2c", "caption 2d", "caption 2e"],
#     # ...
# }


class Flickr8kDataset(Dataset):
    """
    Dataset t√πy ch·ªânh ƒë·ªÉ t·∫£i ·∫£nh v√† ch√∫ th√≠ch cho m√¥ h√¨nh CLIP.
    """
    def __init__(self, root_dir, captions_dict, processor):
        self.root_dir = root_dir
        self.processor = processor

        # LOGIC M·ªöI: M·ªü r·ªông dataset
        expanded_items = []
        for img_file, caps in captions_dict.items():
            # Duy·ªát qua T·∫§T C·∫¢ c√°c caption c·ªßa m·ªôt ·∫£nh
            for caption in caps:
                # Th√™m t·ª´ng c·∫∑p (t√™n_t·ªáp_·∫£nh, ch√∫_th√≠ch_duy_nh·∫•t) v√†o danh s√°ch m·ªõi
                expanded_items.append((img_file, caption))

        self.items = expanded_items # G√°n danh s√°ch ƒë√£ m·ªü r·ªông

    def __len__(self):
        """Tr·∫£ v·ªÅ t·ªïng s·ªë ·∫£nh trong dataset."""
        return len(self.items)

    def __getitem__(self, idx):
        """
        L·∫•y m·ªôt m·∫´u d·ªØ li·ªáu (·∫£nh v√† ch√∫ th√≠ch) t·∫°i ch·ªâ m·ª•c idx,
        sau ƒë√≥ ti·ªÅn x·ª≠ l√Ω ch√∫ng b·∫±ng CLIPProcessor.
        """
        img_file, caps = self.items[idx]
        img_path = os.path.join(self.root_dir, img_file)
        try:
            image = Image.open(img_path).convert("RGB")
        except (FileNotFoundError, UnidentifiedImageError):
            # üîÅ N·∫øu l·ªói, ch·ªçn 1 ·∫£nh kh√°c (ƒë·ªá quy)
            return self.__getitem__((idx + 1) % len(self.items))

        # Ti·ªÅn x·ª≠ l√Ω ·∫£nh v√† vƒÉn b·∫£n b·∫±ng CLIPProcessor
        inputs = self.processor(
          text=caps,
          images=image,
          return_tensors="pt",
          padding='max_length', # <--- Thay ƒë·ªïi: B·∫Øt bu·ªôc padding t·ªõi max length
          truncation=True,
          max_length=77        # <--- Th√™m ƒë·ªô d√†i t·ªëi ƒëa c·ªßa CLIP
        )

        # Lo·∫°i b·ªè chi·ªÅu batch size d∆∞ th·ª´a (k√≠ch th∆∞·ªõc 1) m√† processor th√™m v√†o
        return {k: v.squeeze(0) for k, v in inputs.items()}

# ----------------------------------------------------------------------
# KH·ªûI T·∫†O PROCESSOR
# ----------------------------------------------------------------------
from torch.utils.data import random_split, DataLoader
from transformers import CLIPProcessor

processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

# ----------------------------------------------------------------------
# KH·ªûI T·∫†O DATASET
# ----------------------------------------------------------------------
dataset = Flickr8kDataset(
    "/content/drive/MyDrive/text_img/imgresize",
    captions_dict,
    processor
)

# ----------------------------------------------------------------------
# CHIA 3 T·∫¨P: TRAIN ‚Äì VAL ‚Äì TEST
# ----------------------------------------------------------------------
total_size = len(dataset)

train_ratio = 0.80
val_ratio   = 0.10
test_ratio  = 0.10

train_size = int(train_ratio * total_size)
val_size   = int(val_ratio   * total_size)
test_size  = total_size - train_size - val_size

train_dataset, val_dataset, test_dataset = random_split(
    dataset,
    [train_size, val_size, test_size]
)

# ----------------------------------------------------------------------
# T·∫†O DATALOADER
# ----------------------------------------------------------------------
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
val_loader   = DataLoader(val_dataset,   batch_size=16, shuffle=False)
test_loader  = DataLoader(test_dataset,  batch_size=16, shuffle=False)

# ----------------------------------------------------------------------
# IN TH·ªêNG K√ä
# ----------------------------------------------------------------------
print("üìå T·ªïng ·∫£nh:", total_size)

print("\n--- TRAIN ---")
print("S·ªë ·∫£nh train:", len(train_dataset))
print("S·ªë batches train:", len(train_loader))

print("\n--- VAL ---")
print("S·ªë ·∫£nh val:", len(val_dataset))
print("S·ªë batches val:", len(val_loader))

print("\n--- TEST ---")
print("S·ªë ·∫£nh test:", len(test_dataset))
print("S·ªë batches test:", len(test_loader))

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
import os
import numpy as np

import torch
model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
model.train().to("cuda")

import torch
# ... (c√°c ƒëo·∫°n code kh√°c, bao g·ªìm c·∫£ kh·ªüi t·∫°o model)

# 1. ƒê·ªãnh nghƒ©a thi·∫øt b·ªã (Colab m·∫∑c ƒë·ªãnh l√† 'cuda' n·∫øu c√≥ GPU)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 2. CHUY·ªÇN M√î H√åNH L√äN GPU
model.to(device)

from tqdm import tqdm
import torch.nn.functional as F
import matplotlib.pyplot as plt

device = "cuda"

optimizer = torch.optim.AdamW(model.parameters(), lr=1e-6)
epochs = 3

train_losses = []
val_losses = []

for epoch in range(epochs):

    # --------------------------
    # TRAINING
    # --------------------------
    model.train()
    total_train_loss = 0

    for batch in tqdm(train_loader, desc=f"Epoch {epoch+1} [TRAIN]"):
        batch = {k: v.to(device) for k, v in batch.items()}

        outputs = model(**batch)

        # T√≠nh loss n·∫øu model kh√¥ng t·ª± tr·∫£ v·ªÅ
        if outputs.loss is None:
            logits_img = outputs.logits_per_image
            logits_txt = outputs.logits_per_text
            labels = torch.arange(len(logits_img), device=device)
            loss_i = F.cross_entropy(logits_img, labels)
            loss_t = F.cross_entropy(logits_txt, labels)
            loss = (loss_i + loss_t) / 2
        else:
            loss = outputs.loss

        total_train_loss += loss.item()

        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

    avg_train_loss = total_train_loss / len(train_loader)
    train_losses.append(avg_train_loss)
    print(f"\nEpoch {epoch+1} TRAIN LOSS: {avg_train_loss:.4f}")


    # --------------------------
    # VALIDATION
    # --------------------------
    model.eval()
    total_val_loss = 0

    with torch.no_grad():
        for batch in tqdm(val_loader, desc=f"Epoch {epoch+1} [VAL]"):
            batch = {k: v.to(device) for k, v in batch.items()}

            outputs = model(**batch)

            if outputs.loss is None:
                logits_img = outputs.logits_per_image
                logits_txt = outputs.logits_per_text
                labels = torch.arange(len(logits_img), device=device)
                loss_i = F.cross_entropy(logits_img, labels)
                loss_t = F.cross_entropy(logits_txt, labels)
                loss = (loss_i + loss_t) / 2
            else:
                loss = outputs.loss

            total_val_loss += loss.item()

    avg_val_loss = total_val_loss / len(val_loader)
    val_losses.append(avg_val_loss)
    print(f"Epoch {epoch+1} VAL LOSS:   {avg_val_loss:.4f}\n")

# Import necessary library
from transformers import CLIPModel, CLIPProcessor
model.save_pretrained("/content/drive/MyDrive/clip_flickr8k_finetuned")
processor.save_pretrained("/content/drive/MyDrive/clip_flickr8k_finetuned")
print("‚úÖ ƒê√£ l∆∞u model fine-tune xong!")

plt.figure(figsize=(8,5))
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("CLIP Fine-tuning Loss Curve")
plt.legend()
plt.grid(True)
plt.savefig("training_curve.png", dpi=300)
plt.close()

plt.figure(figsize=(8,5))
plt.plot(train_losses, label="Train Loss")
plt.plot(val_losses, label="Validation Loss")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title("CLIP Fine-tuning Loss Curve")
plt.legend()
plt.grid(True)
plt.show()

model = CLIPModel.from_pretrained("/content/drive/MyDrive/clip_flickr8k_finetuned").to(device)
processor = CLIPProcessor.from_pretrained("/content/drive/MyDrive/clip_flickr8k_finetuned")

def evaluate_test_loss(model, test_loader, device="cuda"):
    model.eval()
    total_loss = 0
    num_batches = 0

    with torch.no_grad():
        for batch in test_loader:
            batch = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**batch)

            # Th√™m logic t√≠nh to√°n loss n·∫øu outputs.loss l√† None
            if outputs.loss is None:
                logits_img = outputs.logits_per_image
                logits_txt = outputs.logits_per_text
                labels = torch.arange(len(logits_img), device=device)
                loss_i = F.cross_entropy(logits_img, labels)
                loss_t = F.cross_entropy(logits_txt, labels)
                loss = (loss_i + loss_t) / 2
            else:
                loss = outputs.loss

            total_loss += loss.item()
            num_batches += 1

    avg_loss = total_loss / num_batches
    return avg_loss

import numpy as np
from tqdm import tqdm

def compute_embeddings(model, test_loader, device="cuda"):
    model.eval()
    all_image_embeds = []
    all_text_embeds = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="Embedding Test Data"):
            batch = {k: v.to(device) for k, v in batch.items()}

            image_embeds = model.get_image_features(batch["pixel_values"])
            text_embeds  = model.get_text_features(batch["input_ids"])

            all_image_embeds.append(image_embeds)
            all_text_embeds.append(text_embeds)

    all_image_embeds = torch.cat(all_image_embeds)
    all_text_embeds  = torch.cat(all_text_embeds)

    # chu·∫©n h√≥a
    all_image_embeds = all_image_embeds / all_image_embeds.norm(dim=-1, keepdim=True)
    all_text_embeds  = all_text_embeds / all_text_embeds.norm(dim=-1, keepdim=True)

    return all_image_embeds, all_text_embeds

def recall_at_k(similarity_matrix, k=1):
    # H√†ng = ·∫£nh, C·ªôt = caption
    ranks = similarity_matrix.argsort(dim=1, descending=True)
    top_k = ranks[:, :k]
    targets = torch.arange(len(ranks)).unsqueeze(1).to(ranks.device)
    correct = (top_k == targets).any(dim=1).float().mean().item()
    return correct

def evaluate_clip_on_test(model, test_loader, device="cuda"):

    print("\nüîé B·∫Øt ƒë·∫ßu Embedded test...")
    img_emb, txt_emb = compute_embeddings(model, test_loader, device)

    # similarity
    sim = img_emb @ txt_emb.T

    # compute retrieval
    r1  = recall_at_k(sim, 1)
    r5  = recall_at_k(sim, 5)
    r10 = recall_at_k(sim, 10)

    print("\nüéØ TEST RETRIEVAL RESULTS")
    print(f"Image ‚Üí Text  : R@1={r1:.4f}, R@5={r5:.4f}, R@10={r10:.4f}")

    loss = evaluate_test_loss(model, test_loader, device)
    print(f"üìâ Test Loss: {loss:.4f}")

    return {
        "test_loss": loss,
        "r1": r1,
        "r5": r5,
        "r10": r10,
    }

results = evaluate_clip_on_test(model, test_loader)