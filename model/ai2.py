# -*- coding: utf-8 -*-
"""AI2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BfFaewPebpQhHMZPAgqwV1RfRLJONG9B
"""

from google.colab import drive
drive.mount('/content/drive')

import torch
print(torch.cuda.is_available())  # True n·∫øu GPU ƒë∆∞·ª£c k√≠ch ho·∫°t
print(torch.cuda.get_device_name(0))  # T√™n GPU, v√≠ d·ª•: Tesla T4

# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
import os
import numpy as np

device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

model = CLIPModel.from_pretrained("/content/drive/MyDrive/clip_flickr8k_finetuned").to(device)
processor = CLIPProcessor.from_pretrained("/content/drive/MyDrive/clip_flickr8k_finetuned")

"""Chu·∫©n h√≥a ·∫£nh (Normalization)
ƒê√¢y l√† chu·∫©n h√≥a ·∫£nh gi·ªëng nh∆∞ CLIP ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán.

N√≥ gi√∫p vector ·∫£nh ƒë·∫ßu ra n·∫±m trong kh√¥ng gian gi·ªëng nh∆∞ l√∫c train model.

ToTensor() chuy·ªÉn ·∫£nh ‚Üí tensor PyTorch

Normalize() chu·∫©n h√≥a gi√° tr·ªã pixel theo mean/std chu·∫©n CLIP.
"""

from torchvision import transforms

CLIP_MEAN = [0.48145466, 0.4578275, 0.40821073]
CLIP_STD = [0.26862954, 0.26130258, 0.27577711]

transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean=CLIP_MEAN, std=CLIP_STD),
])

# H√†m tr√≠ch xu·∫•t vector t·ª´ ·∫£nh
def extract_image_features(image_path):
    image = Image.open(image_path) #.convert('RGB')
    image_tensor = transform(image).unsqueeze(0).to(device)
    with torch.no_grad():
      image_features = model.get_image_features(pixel_values=image_tensor)

    image_features = image_features / image_features.norm(p=2, dim=-1, keepdim=True)
    return image_features.cpu().numpy()

# ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a ·∫£nh tr√™n Google Drive
image_dir = "/content/drive/MyDrive/text_img/imgresize/"
output_file = "/content/drive/MyDrive/text_img/image_vectors.npy"

# X·ª≠ l√Ω t·∫•t c·∫£ ·∫£nh v√† l∆∞u vector
image_features_dict = {}
for filename in os.listdir(image_dir):
  file_path = os.path.join(image_dir, filename)
  if file_path.endswith((".jpg", ".png", ".jpeg")):
      try:
          feature = extract_image_features(file_path)
          image_features_dict[file_path] = feature
          if len(image_features_dict) % 100 == 0:  # In ti·∫øn ƒë·ªô sau m·ªói 100 ·∫£nh
              print(f"Processed {len(image_features_dict)} images")
      except Exception as e:
          print(f"Error processing {file_path}: {e}")
# L∆∞u k·∫øt qu·∫£ v√†o file .npy tr√™n Google Drive
np.save(output_file, image_features_dict)
print(f"Completed! Saved {len(image_features_dict)} image vectors to {output_file}")

# x·ª≠ l√Ω vecto text
def extract_text_features(query):
    inputs = processor(text=[query], return_tensors="pt", padding=True).to(device)
    with torch.no_grad():
        text_features = model.get_text_features(**inputs)
    text_features = text_features / text_features.norm(p=2, dim=-1, keepdim=True)
    return text_features.cpu().numpy()

loaded = np.load("/content/drive/MyDrive/text_img/image_vectors.npy", allow_pickle=True).item()

print("S·ªë ·∫£nh:", len(loaded))
print("M·ªôt key th·ª≠:", list(loaded.keys())[:5])

def cosine_similarity(a, b):
    a = np.array(a).squeeze()
    b = np.array(b).squeeze()
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))

scores = []
query_vec = extract_text_features("A boy jumping in a fountain ")
for img_path, img_vec in loaded.items():
    img_vec = img_vec.reshape(1, -1)
    score = float(cosine_similarity([img_vec], [query_vec]))
    scores.append((img_path, score))

# S·∫Øp x·∫øp gi·∫£m d·∫ßn
scores.sort(key=lambda x: x[1], reverse=True)
top_k = 5
for img, s in scores[:top_k]:
    print(img, s)

# ƒê∆∞·ªùng d·∫´n ·∫£nh
path = "/content/drive/MyDrive/text_img/imgresize/3758175529_81941e7cc9.jpg"

"""PH·∫¶N TRUY V·∫§N ·∫¢NH B·∫∞NG C·∫¢ TEXT V√Ä IMAGE"""

def combine_query(text_vec=None, image_vec=None, alpha=0.6):
    """
    alpha = tr·ªçng s·ªë text
    (1-alpha) = tr·ªçng s·ªë ·∫£nh

    N·∫øu ch·ªâ c√≥ text ho·∫∑c ch·ªâ c√≥ ·∫£nh ‚Üí t·ª± x·ª≠ l√Ω.
    """
    if text_vec is not None and image_vec is not None:
        combined = alpha * text_vec + (1 - alpha) * image_vec
    elif text_vec is not None:
        combined = text_vec
    else:
        combined = image_vec

    combined = combined / np.linalg.norm(combined)
    return combined

# H√†m ti·ªÅn s·ª≠ l√Ω ·∫£nh ƒë·∫ßu v√†o
import cv2
def preprocess_image(image_path):

    try:
        # M·ªü v√† chuy·ªÉn ƒë·ªïi ·∫£nh sang ƒë·ªãnh d·∫°ng RGB
        image = cv2.imread(image_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        h, w = image.shape[:2]

        # t·∫°o ·∫£nh n·ªÅn vu√¥ng c√≥ c·∫°nh b·∫±ng max(h, w)
        size = max(h, w)
        square = np.zeros((size, size, 3), dtype=np.uint8)

        # t√≠nh offset ƒë·ªÉ cƒÉn gi·ªØa
        y_offset = (size - h) // 2
        x_offset = (size - w) // 2

        # ch√®n ·∫£nh g·ªëc v√†o
        square[y_offset:y_offset+h, x_offset:x_offset+w] = image
        # resize
        inputs = cv2.resize(square, (224, 224))



        return inputs
    except Exception as e:
        print(f"L·ªói khi ti·ªÅn x·ª≠ l√Ω ·∫£nh {image_path}: {e}")
        return None

# V√≠ d·ª• s·ª≠ d·ª•ng
# image_path = "/content/drive/MyDrive/dataset/messi/img_1.jpg"
# processed_image = preprocess_image(image_path)
# if processed_image:
#     print("·∫¢nh ƒë√£ ƒë∆∞·ª£c ti·ªÅn x·ª≠ l√Ω th√†nh c√¥ng!")

text_query = "ƒë√¢y l√† text_query"
text_vec = extract_text_features(text_query)
path_img = "ƒë√¢y l√† ƒë∆∞·ªùng d·∫´n ·∫£nh query"
image_vec = extract_image_features(path_img)
combined_vec = combine_query(text_vec, image_vec)
scores = []
for img_path, img_vec in loaded.items():
    img_vec = img_vec.reshape(1, -1)
    score = float(cosine_similarity([img_vec], [combined_vec]))
    scores.append((img_path, score))

scores.sort(key=lambda x: x[1], reverse=True)
top_k = 5
for img, s in scores[:top_k]:
    print(img, s)

pp = "/content/drive/MyDrive/text_img/imgresize/3280672302_2967177653.jpg"

"""END"""

# 1. C√†i ƒë·∫∑t th∆∞ vi·ªán Server & Tunnel
!pip install fastapi uvicorn pyngrok python-multipart nest-asyncio deep-translator

# 2. K·∫øt n·ªëi v·ªõi Model v√† Database ƒë√£ load s·∫µn
import sys

# Ki·ªÉm tra xem b·∫°n ƒë√£ ch·∫°y c√°c cell b√™n tr√™n ch∆∞a
if 'model' not in globals() or 'processor' not in globals():
    print("‚ùå L·ªñI: B·∫°n ch∆∞a ch·∫°y c√°c cell load Model b√™n tr√™n. H√£y ch·∫°y l·∫°i notebook t·ª´ ƒë·∫ßu!")
elif 'loaded' not in globals():
    print("‚ùå L·ªñI: B·∫°n ch∆∞a ch·∫°y cell load Database (bi·∫øn 'loaded').")
else:
    # Map bi·∫øn 'loaded' c·ªßa b·∫°n sang t√™n 'vectors_db' ƒë·ªÉ Server d√πng
    vectors_db = loaded

    print(f"‚úÖ ƒê√£ k·∫øt n·ªëi Model CLIP (Device: {model.device})")
    print(f"‚úÖ ƒê√£ k·∫øt n·ªëi Database: {len(vectors_db)} vectors")
    print("üöÄ S·∫µn s√†ng kh·ªüi ƒë·ªông Server!")

# Cell 3: Kh·ªüi t·∫°o FastAPI Server (ƒê√£ Fix l·ªói 'Image object has no attribute read')
from fastapi import FastAPI, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from deep_translator import GoogleTranslator
import io
import base64
import numpy as np
import torch
from PIL import Image

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

translator = GoogleTranslator(source='auto', target='en')

def image_to_base64(path):
    try:
        if not os.path.exists(path): return None
        with open(path, "rb") as img_file:
            return "data:image/jpeg;base64," + base64.b64encode(img_file.read()).decode('utf-8')
    except:
        return None

@app.post("/predict")
async def predict(
    file: UploadFile = File(None),
    query: str = Form(None),
    alpha: float = Form(0.5)
):
    print(f"üì• Nh·∫≠n request: Text='{query}', ·∫¢nh={'C√≥' if file else 'Kh√¥ng'}")

    text_vec = None
    image_vec = None

    try:
        # 1. X·ª≠ l√Ω Text
        if query:
            translated = translator.translate(query)
            print(f"   D·ªãch: '{query}' -> '{translated}'")
            # H√†m n√†y OK v√¨ n√≥ nh·∫≠n string
            text_vec = extract_text_features(translated)

        # 2. X·ª≠ l√Ω ·∫¢nh (FIX L·ªñI T·∫†I ƒê√ÇY)
        if file:
            content = await file.read()
            pil_img = Image.open(io.BytesIO(content)).convert("RGB")

            # --- CODE S·ª¨A ƒê·ªîI: Tr√≠ch xu·∫•t vector tr·ª±c ti·∫øp ---
            # Thay v√¨ g·ªçi extract_image_features(pil_img), ta ch·∫°y l·ªánh transform tr·ª±c ti·∫øp
            img_tensor = transform(pil_img).unsqueeze(0).to(device)
            with torch.no_grad():
                feat = model.get_image_features(pixel_values=img_tensor)

            # Chu·∫©n h√≥a vector
            feat = feat / feat.norm(p=2, dim=-1, keepdim=True)
            image_vec = feat.cpu().numpy()
            print("   -> ƒê√£ t·∫°o vector ·∫£nh th√†nh c√¥ng")

        # 3. K·∫øt h·ª£p (Hybrid Search)
        if text_vec is not None or image_vec is not None:
            # Chuy·ªÉn ƒë·ªïi v·ªÅ tensor ƒë·ªÉ d√πng h√†m combine (n·∫øu c·∫ßn) ho·∫∑c c·ªông tr·ª±c ti·∫øp numpy
            # ƒê·ªÉ ƒë∆°n gi·∫£n v√† kh·ªõp v·ªõi h√†m combine_query c≈©, ta √©p ki·ªÉu v·ªÅ Tensor n·∫øu h√†m ƒë√≥ y√™u c·∫ßu,
            # ho·∫∑c t√≠nh to√°n th·ªß c√¥ng ngay t·∫°i ƒë√¢y ƒë·ªÉ an to√†n nh·∫•t:

            v1 = text_vec if text_vec is not None else np.zeros((1, 512))
            v2 = image_vec if image_vec is not None else np.zeros((1, 512))

            # N·∫øu c·∫£ 2 ƒë·ªÅu c√≥ d·ªØ li·ªáu
            if text_vec is not None and image_vec is not None:
                final_vec = alpha * v1 + (1 - alpha) * v2
            elif text_vec is not None:
                final_vec = v1
            else:
                final_vec = v2

            # Chu·∫©n h√≥a l·∫ßn cu·ªëi
            norm = np.linalg.norm(final_vec)
            if norm > 0:
                final_vec = final_vec / norm

            # ƒê·∫£m b·∫£o shape ƒë√∫ng (1, 512)
            final_vec = final_vec.reshape(1, -1)

            # 4. T√¨m ki·∫øm
            scores = []
            for path, vec in vectors_db.items():
                vec = vec.reshape(1, -1)
                # T√≠nh Cosine Similarity
                score = np.dot(final_vec, vec.T).item()
                scores.append((path, float(score)))

            scores.sort(key=lambda x: x[1], reverse=True)

            results = []
            for path, score in scores[:5]:
                b64 = image_to_base64(path)
                if b64: results.append({"image": b64, "score": score})

            print(f"‚úÖ Tr·∫£ v·ªÅ {len(results)} k·∫øt qu·∫£")
            return {"top5": results}

    except Exception as e:
        print(f"‚ùå L·ªói: {e}")
        import traceback
        traceback.print_exc() # In chi ti·∫øt l·ªói ra log Colab ƒë·ªÉ debug
        return {"error": str(e)}

    return {"error": "Kh√¥ng c√≥ d·ªØ li·ªáu ƒë·∫ßu v√†o"}

# Ch·∫°y Server v√† Public qua Ngrok
import uvicorn
import nest_asyncio
from pyngrok import ngrok

# --- ƒêI·ªÄN TOKEN C·ª¶A B·∫†N V√ÄO ƒê√ÇY ---
NGROK_TOKEN = "35txrgcicOrnCwU6m0F6zWoDvm6_3uaLFqCTzWNyqmnCgDCCx"
ngrok.set_auth_token(NGROK_TOKEN)

# M·ªü tunnel port 8000
ngrok_tunnel = ngrok.connect(8000)
print('üöÄ PUBLIC URL C·ª¶A B·∫†N:', ngrok_tunnel.public_url)
print('üëâ H√£y copy URL n√†y d√°n v√†o file config.py ·ªü backend!')

# Ch·∫°y server
nest_asyncio.apply()

# C·∫•u h√¨nh server th·ªß c√¥ng
config = uvicorn.Config(app, host="0.0.0.0", port=8000)
server = uvicorn.Server(config)

# Ch·∫°y server b·∫±ng await (t∆∞∆°ng th√≠ch v·ªõi Colab)
await server.serve()